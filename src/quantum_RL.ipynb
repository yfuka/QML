{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from libs.rlinml.agent import Agent\n",
    "from utils import prepare_device, prepare_env, save_reward_change, load_reward_change, plot_reward_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from itertools import count\n",
    "import qiskit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_name:cpu\n"
     ]
    }
   ],
   "source": [
    "# prepare env\n",
    "env = prepare_env('CartPole-v1')\n",
    "\n",
    "# prepare device\n",
    "device, device_name = prepare_device()\n",
    "\n",
    "if device_name == \"cuda\":\n",
    "    print(\"device_name:cuda\")\n",
    "    num_episodes = 6000\n",
    "else:\n",
    "    print(\"device_name:cpu\")\n",
    "    num_episodes = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial parameters of the nets:\n",
      "OrderedDict([('custom_multiply_layer1.weight', tensor([1., 1., 1., 1.])), ('re_uploading_PQC_layer.thetas', tensor([0.2145, 1.7541, 1.8549, 0.3300, 2.0042, 1.1117, 1.0628, 2.3753, 1.0560,\n",
      "        0.9189, 0.7430, 1.8682, 0.6279, 1.0090, 3.0084, 2.1423, 2.4867, 0.0782,\n",
      "        0.0432, 2.5322, 0.0427, 1.3226, 2.7323, 0.0964, 1.8358, 0.4877, 1.9398,\n",
      "        1.2508, 0.6110, 2.7681, 0.5107, 2.5807, 1.3975, 0.7577, 1.6016, 2.0821,\n",
      "        1.6162, 2.9922, 0.2042, 0.1317, 1.0983, 2.3725, 0.6005, 0.2829, 1.4809,\n",
      "        2.4460, 1.2713, 0.1079, 0.3398, 0.1672, 0.6277, 2.4331, 3.0493, 0.5349,\n",
      "        1.3874, 2.6574, 0.4520, 1.4431, 2.7301, 2.1936, 1.9479, 1.9185, 0.7471,\n",
      "        2.8767, 0.5414, 0.8636, 1.2191, 2.0101, 2.6371, 0.2666, 1.5765, 2.2905])), ('custom_multiply_layer2.weight', tensor([1., 1.]))])\n",
      "The epislon was initialized to 1.0\n"
     ]
    }
   ],
   "source": [
    "hyper_params = {'GAMMA':0.99, 'BATCH_SIZE':16, 'MEMORY_SIZE':10000, 'c_depth': 5, \\\n",
    "                'backend': qiskit.Aer.get_backend('statevector_simulator'), 'shots':1}\n",
    "model_type = \"hybrid\"\n",
    "\n",
    "agent = Agent(env, hyper_params, model_type, device)\n",
    "\n",
    "total_steps = 0\n",
    "STEPS_PER_UPDATE = 10 #1 # Train the model every x steps\n",
    "STEPS_PER_TARGET_UPDATE = 30 #1 # Update the target model every x steps\n",
    "reward_change = [] # あるエピソードにおいて、どの程度期間カートポールを立てていられていたか\n",
    "\n",
    "epsilon = 1.0 # Epsilon greedy parameter\n",
    "print(\"The epislon was initialized to \" + str(epsilon))\n",
    "epsilon_min = 0.01  # Minimum epsilon greedy parameter\n",
    "decay_epsilon = 0.99 # Decay rate of epsilon greedy parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get it's state\n",
    "    state = env.reset()[0]\n",
    "\n",
    "    for t in count():\n",
    "        action = agent.get_action(state, epsilon)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = observation\n",
    "\n",
    "        # Store the transition in memory\n",
    "        agent.store_experience(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        total_steps += 1\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        if total_steps % STEPS_PER_UPDATE == 0:\n",
    "            agent.optimize_model()\n",
    "\n",
    "        # update of the target network's weights\n",
    "        if total_steps % STEPS_PER_TARGET_UPDATE == 0:\n",
    "            agent.update_target_network()\n",
    "\n",
    "        if done:\n",
    "            # Decay epsilon\n",
    "            epsilon = max(epsilon * decay_epsilon, epsilon_min)\n",
    "            reward_change.append(t+1)\n",
    "            break\n",
    "\n",
    "    if (i_episode+1)%10 == 0:\n",
    "        avg_rewards = np.mean(reward_change[-10:])\n",
    "        print(\"Episode {}/{}, average last 10 rewards {}\".format(i_episode+1, num_episodes, avg_rewards))\n",
    "        if avg_rewards >= 500.0:\n",
    "            break\n",
    "\n",
    "end = time.time()\n",
    "print('Complete')\n",
    "print(\"It takes {} minitues\".format((end-start)/60))\n",
    "\n",
    "print(os.getcwd())\n",
    "PATH = os.getcwd()\n",
    "save_reward_change(reward_change, PATH, file_name=\"reward_change_hybrid.txt\")\n",
    "agent.save_model(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reward_change(reward_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_change_loaded = load_reward_change(PATH, file_name=\"reward_change_hybrid.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reward_change_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a982ae48139485ecdd4f00aefd1c1e9fd123c801b7566071bdb9698e10973568"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
